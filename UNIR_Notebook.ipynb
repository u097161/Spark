{"cells":[{"cell_type":"code","source":["En la actividad se pide que se importen los dos ficheros, cs_test y cs_training y asi lo realizaremos. Sin embargo, para entrenar y testar el modelo partiremos exclusivamente de cs_training ya que cs_test no tiene la variable objetivo informada."],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# Loading CSV files from DBFS into RDDs in cluster memory\n\ncs_test = sc.textFile('/FileStore/tables/0itamgtk1506008427622/cs_test-31b97.csv')\ncs_training = sc.textFile('/FileStore/tables/94yylx7v1506008660804/cs_training-d35cb.csv')"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# See what we've got in the RDDs\n\nprint('--- Test:')\nprint(cs_test.take(4))\nprint('--- Training:')\nprint(cs_training.take(4))"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Getting the Spark SQL context and imports\nfrom pyspark.sql import SQLContext, Row\nsqlContext = SQLContext.getOrCreate(sc.getOrCreate())"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Reading CSV to a df, infering the schema\ntestDatawithoutlabels = sqlContext.read.format(\"com.databricks.spark.csv\") \\\n  .option(\"header\", \"true\").option(\"delimiter\",\",\").option(\"inferschema\", \"true\") \\\n  .load(\"/FileStore/tables/0itamgtk1506008427622/cs_test-31b97.csv\")\n  \ntrainingData = sqlContext.read.format(\"com.databricks.spark.csv\") \\\n  .option(\"header\", \"true\").option(\"delimiter\",\",\").option(\"inferschema\", \"true\") \\\n  .load(\"/FileStore/tables/94yylx7v1506008660804/cs_training-d35cb.csv\")\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Check schema\nprint('--- Test:')\ntestDatawithoutlabels.printSchema()\nprint('--- Training:')\ntrainingData.printSchema()\n\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#even though the columns MonthlyIncome and NumberOfDependents are numeric they are infered as as strings.\n# The following call takes all columns (df.columns) and casts them using Spark SQL to a numeric type (DoubleType).\nfrom pyspark.sql.functions import col  # for indicating a column using a string in the line below\ntestDatawithoutlabels = testDatawithoutlabels.select([col(c).cast(\"double\").alias(c) for c in testDatawithoutlabels.columns])\ntestDatawithoutlabels.printSchema()\n\n\n\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["#even though the columns MonthlyIncome and NumberOfDependents are numeric they are infered as as strings.\n# The following call takes all columns (df.columns) and casts them using Spark SQL to a numeric type (DoubleType).\ntrainingData = trainingData.select([col(c).cast(\"double\").alias(c) for c in trainingData.columns])\ntrainingData.printSchema()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Check data\ndisplay(trainingData) "],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["trainingData.count()\n"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["testDatawithoutlabels.count()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":[" # drop rows with missing values\ntrainingData = trainingData.dropna()\ntrainingData.count()\n\n\n"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["display(trainingData)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["#realizamos consultas SQL para ver la calidad de los datos\ntrainingData.registerTempTable(\"trainingData\")\ntrainingData.count()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["%sql select min(age), max(age) from trainingData"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%sql select count(*) from trainingData where age <18"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["#eliminamos registro con cero aÃ±os\ntrainingData=sqlContext.sql(\"SELECT * FROM trainingData WHERE age >18\")"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["trainingData.count()\n"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["testDatawithoutlabels2 = testDatawithoutlabels.drop(\"SeriousDlqin2yrs\")\ndisplay(testDatawithoutlabels2)\n"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["testDatawithoutlabels2 = testDatawithoutlabels2.dropna()\ntestDatawithoutlabels2.count()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["display(testDatawithoutlabels2)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["#We are going to use trainingData, and build from this dataframe the test data. The original test_cs data does not have label column to verify #the algorithm accuracy\n\ndf=trainingData\n# Split the dataset randomly into 70% for training and 30% for testing.\ntrain, test = df.randomSplit([0.7, 0.3])\nprint \"We have %d training examples and %d test examples.\" % (train.count(), test.count())"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler, VectorIndexer\nfeaturesCols = df.columns\nfeaturesCols.remove('SeriousDlqin2yrs')\n# This concatenates all feature columns into a single feature vector in a new column \"rawFeatures\".\nvectorAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"rawFeatures\")\n# This identifies categorical features and indexes them.\nvectorIndexer = VectorIndexer(inputCol=\"rawFeatures\", outputCol=\"features\", maxCategories=4)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["# We are going to use the GBTRegressor algorithm. Gradient boosting is a machine learning technique for regression and classification problems, #which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a #stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss #cfunction.\nfrom pyspark.ml.regression import GBTRegressor\n# Takes the \"features\" column and learns to predict \"cnt\"\ngbt = GBTRegressor(labelCol=\"SeriousDlqin2yrs\")"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import RegressionEvaluator\n# Define a grid of hyperparameters to test:\n#  - maxDepth: max depth of each decision tree in the GBT ensemble\n#  - maxIter: iterations, i.e., number of trees in each GBT ensemble\n# In this example notebook, we keep these values small.  In practice, to get the highest accuracy, you would likely want to try deeper trees (10 or higher) and more trees in the ensemble (>100).\nparamGrid = ParamGridBuilder()\\\n  .addGrid(gbt.maxDepth, [2, 5])\\\n  .addGrid(gbt.maxIter, [10, 100])\\\n  .build()\n# We define an evaluation metric.  This tells CrossValidator how well we are doing by comparing the true labels with predictions.\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=gbt.getLabelCol(), predictionCol=gbt.getPredictionCol())\n# Declare the CrossValidator, which runs model tuning for us.\ncv = CrossValidator(estimator=gbt, evaluator=evaluator, estimatorParamMaps=paramGrid)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["from pyspark.ml import Pipeline\npipeline = Pipeline(stages=[vectorAssembler, vectorIndexer, cv])"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["pipelineModel = pipeline.fit(train)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["\npredictions = pipelineModel.transform(test)\n"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["display(predictions.select(\"SeriousDlqin2yrs\", \"prediction\", *featuresCols))\n"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["rmse = evaluator.evaluate(predictions)\nprint \"RMSE on our test set: %g\" % rmse\n\n#Lower values of RMSE show a better fit"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":33}],"metadata":{"name":"UNIR_Notebook","notebookId":3623268950465994},"nbformat":4,"nbformat_minor":0}
